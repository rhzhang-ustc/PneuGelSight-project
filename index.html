<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="PneuGelSight: Soft Robotic Vision-Based Proprioception and Tactile Sensing - Zhang and Yuan">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="Our work presents a novel vision-based approach for sensorizing soft robots.  To optimize the sensor's performance, we introduce a comprehensive pipeline that accurately simulates its optical and dynamic properties, facilitating a zero-shot knowledge transition from simulation to real-world applications. PneuGelSight and our sim-to-real pipeline provide a novel, easily implementable, and robust sensing methodology for soft robots, paving the way for the development of more advanced soft robots with enhanced sensory capabilities.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="Soft robot, Tactile sensing, Perception, AI">
  <!-- TODO: List all authors -->
  <meta name="author" content="Ruohan Zhang, Uksang Yoo, Yichen Li, Arpit Argawal and Wenzhen Yuan">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="University of Illinois, Urbana-Champaign">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="PneuGelSight: Soft Robotic Vision-Based Proprioception and Tactile Sensing">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="Our work presents a novel vision-based approach for sensorizing soft robots.  To optimize the sensor's performance, we introduce a comprehensive pipeline that accurately simulates its optical and dynamic properties, facilitating a zero-shot knowledge transition from simulation to real-world applications. PneuGelSight and our sim-to-real pipeline provide a novel, easily implementable, and robust sensing methodology for soft robots, paving the way for the development of more advanced soft robots with enhanced sensory capabilities.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://rhzhang-ustc.github.io/PneuGelSight-project/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://rhzhang-ustc.github.io/PneuGelSight-project/static/images/social_preview.jpg">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PneuGelSight - Research Preview">
  <meta property="article:published_time" content="2025-08-16T00:00:00.000Z">
  <meta property="article:author" content="Ruohan Zhang">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="Robotics">
  <meta property="article:tag" content="Tactile sensing">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="PneuGelSight: Soft Robotic Vision-Based Proprioception and Tactile Sensing">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="Our work presents a novel vision-based approach for sensorizing soft robots.  To optimize the sensor's performance, we introduce a comprehensive pipeline that accurately simulates its optical and dynamic properties, facilitating a zero-shot knowledge transition from simulation to real-world applications. PneuGelSight and our sim-to-real pipeline provide a novel, easily implementable, and robust sensing methodology for soft robots, paving the way for the development of more advanced soft robots with enhanced sensory capabilities.">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://rhzhang-ustc.github.io/PneuGelSight-project/static/images/social_preview.jpg">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="PneuGelSight: Soft Robotic Vision-Based Proprioception and Tactile Sensing">
  <meta name="citation_author" content="Zhang, Ruoha ">
  <meta name="citation_author" content="Yoo, Uksang">
  <meta name="citation_author" content="Li, Yichen">
  <meta name="citation_author" content="Argawal, Arpit">
  <meta name="citation_author" content="Yuan, Wenzhen">

  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="The International Journal of Robotics Research">
  <meta name="citation_pdf_url" content="https://rhzhang-ustc.github.io/PneuGelSight-project/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title> PneuGelSight - Ruohan Zhang and Wenzhen Yuan</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PneuGelSight: Soft Robotic Vision-Based Proprioception and Tactile Sensing",
    "description": "Soft pneumatic robot manipulators are popular in industrial and human-interactive applications due to their compliance and flexibility. However, deploying them in real-world scenarios requires advanced sensing for tactile feedback and proprioception. Our work presents a novel vision-based approach for sensorizing soft robots. We demonstrate our approach on PneuGelSight, a pioneering pneumatic manipulator featuring high-resolution proprioception and tactile sensing via an embedded camera. To optimize the sensor's performance, we introduce a comprehensive pipeline that accurately simulates its optical and dynamic properties, facilitating a zero-shot knowledge transition from simulation to real-world applications. PneuGelSight and our sim-to-real pipeline provide a novel, easily implementable, and robust sensing methodology for soft robots, paving the way for the development of more advanced soft robots with enhanced sensory capabilities.",
    "author": [
      {
        "@type": "Person",
        "name": "Ruohan Zhang",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Illinois, Urbana-Champaign"
        }
      },
      {
        "@type": "Person",
        "name": "Uksang Yoo",
        "affiliation": {
          "@type": "Organization",
          "name": "Carnegie Mellon University"
        }
      },
      {
        "@type": "Person",
        "name": "Yichen Li",
        "affiliation": {
          "@type": "Organization",
          "name": "Carnegie Mellon University"
        }
      },
      {
        "@type": "Person",
        "name": "Arpit Argawal",
        "affiliation": {
          "@type": "Organization",
          "name": "Carnegie Mellon University"
        }
      },
      {
        "@type": "Person",
        "name": "Wenzhen Yuan",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Illinois, Urbana-Champaign"
        }
      }
    ],
    "datePublished": "2025-08-25",
    "publisher": {
      "@type": "Organization",
      "name": "arXiv"
    },
    "url": "https://arxiv.org/abs/2508.18443",
    "image": "https://rhzhang-ustc.github.io/PneuGelSight-project/static/images/teaser.png",
    "keywords": [
      "cs",
      "RO",
      "cs.RO"
    ],
    "abstract": "Soft pneumatic robot manipulators are popular in industrial and human-interactive applications due to their compliance and flexibility. However, deploying them in real-world scenarios requires advanced sensing for tactile feedback and proprioception. Our work presents a novel vision-based approach for sensorizing soft robots. We demonstrate our approach on PneuGelSight, a pioneering pneumatic manipulator featuring high-resolution proprioception and tactile sensing via an embedded camera. To optimize the sensor's performance, we introduce a comprehensive pipeline that accurately simulates its optical and dynamic properties, facilitating a zero-shot knowledge transition from simulation to real-world applications. PneuGelSight and our sim-to-real pipeline provide a novel, easily implementable, and robust sensing methodology for soft robots, paving the way for the development of more advanced soft robots with enhanced sensory capabilities.",
    "isAccessibleForFree": true,
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://arxiv.org/abs/2508.18443"
    }
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "RoboTouch Lab",
    "url": "https://robotouchlab.web.illinois.edu",
    "logo": "https://rhzhang-ustc.github.io/PneuGelSight-project/static/images/favicon.ico",
    "sameAs": [
      "https://github.com/rhzhang-ustc"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <!-- TODO: Replace with your lab's related works -->
        <a href="https://arxiv.org/abs/2501.06263" class="work-item" target="_blank">
          <div class="work-info">
            <h5>GelBelt: A Vision-based Tactile Sensor for Continuous Sensing of Large Surfaces</h5>
            <p>A novel tactile sensor using an elastomeric belt and two wheels that can continuously scan large surfaces with high accuracy; shows robust shape reconstruction at speeds up to ~45 mm/s. :contentReference[oaicite:1]{index=1}</p>
            <span class="work-venue">IEEE RA-L, arXiv 2025</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/2412.09617" class="work-item" target="_blank">
          <div class="work-info">
            <h5>NormalFlow: Fast, Robust, and Accurate Contact-based Object 6DoF Pose Tracking with Vision-based Tactile Sensors</h5>
            <p>An algorithm for real-time 6-DoF pose tracking using vision-based tactile sensors via precise normal estimation; shows strong accuracy even on low-texture objects. Based on RA-L 2024.</p>
            <span class="work-venue">IEEE RA-L, arXiv 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">PneuGelSight: Soft Robotic Vision-Based Proprioception and Tactile Sensinge</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://rhzhang-ustc.github.io" target="_blank">Ruohan Zhang</a>,</span>
                  <span class="author-block">
                    Uksang Yoo,
                  </span>
                  <span class="author-block">
                    Yichen Li,
                  </span>
                  <span class="author-block">
                    Arpit Argawal,
                  </span>
                  <span class="author-block">
                    <a href="https://robotouchlab.web.illinois.edu/" target="_blank">Wenzhen Yuan</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block">University of Illinois, Urbana-Champaign<br>The International Journal of Robotics Research, 2025</span>
                    <!-- TODO: Remove this line if no equal contribution -->
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2508.18443.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- TODO: Add your supplementary material PDF or remove this section -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a> -->
                  <!-- </span> -->

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://drive.google.com/drive/folders/1_cb1CJaEQN_CRJUpdzZMATWkf7SGQSAK?usp=share_link" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Design and Simulation Files</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2508.18443" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- TODO: Replace with your teaser video -->
      <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata">
        <!-- TODO: Add your video file path here -->
        <source src="static/videos/banner_video.mp4" type="video/mp4">
      </video>
      <!-- TODO: Replace with your video description -->
      <h2 class="subtitle has-text-centered">
        PneuGelSight in action: a soft pneumatic manipulator equipped with high-resolution vision-based tactile sensing. The embedded camera reconstructs contact geometry in real time, enabling delicate and intelligent interaction with the physical world.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            Soft pneumatic robot manipulators are popular in industrial and human-interactive applications due to their compliance and flexibility. However, deploying them in real-world scenarios requires advanced sensing for tactile feedback and proprioception. Our work presents a novel vision-based approach for sensorizing soft robots. We demonstrate our approach on PneuGelSight, a pioneering pneumatic manipulator featuring high-resolution proprioception and tactile sensing via an embedded camera. To optimize the sensor's performance, we introduce a comprehensive pipeline that accurately simulates its optical and dynamic properties, facilitating a zero-shot knowledge transition from simulation to real-world applications. PneuGelSight and our sim-to-real pipeline provide a novel, easily implementable, and robust sensing methodology for soft robots, paving the way for the development of more advanced soft robots with enhanced sensory capabilities.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- TODO: Replace with your research result images -->
        <img src="static/images/cool demo.png" alt="First research result visualization" loading="lazy"/>
        <!-- TODO: Replace with description of this result -->
        <h2 class="subtitle has-text-centered">
          Pipeline for estimating an object's shape and texture through multiple touches. We mount our gripper on a UR5e robot arm, rotating it to generate different touching poses. The reconstructed surface texture is then stitched together for the final prediction.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/tactile sensing.png" alt="Second research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
          Tactile Imprints of an avocado under different bending angles, along with the corresponding color differences, predicted surface normals, and reconstructed 3D meshes. 
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/proprioception.png" alt="Third research result visualization" loading="lazy"/>
        <h2 class="subtitle has-text-centered">
         Result of high-resolution proprioception. We categorize the gripper’s deformation into five representative scenarios and present the corresponding external view (top row), internal camera view (middle row), and predicted point clouds (bottom row) from two viewpoints. The point clouds are color-coded based on the Z-axis values.  The Chamfer Distances between the predicted and ground truth point clouds are reported for each case to quantify reconstruction accuracy.
       </h2>
     </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- TODO: Replace with your YouTube video ID -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1"> -->
          <!-- TODO: Add poster image for better preview -->
          <!-- <video poster="" id="video1" controls muted loop height="100%" preload="metadata"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video2"> -->
          <!-- TODO: Add poster image for better preview -->
          <!-- <video poster="" id="video2" controls muted loop height="100%" preload="metadata"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel2.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item item-video3"> -->
          <!-- TODO: Add poster image for better preview -->
          <!-- <video poster="" id="video3" controls muted loop height="100%" preload="metadata"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel3.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Paper</h2>

      <!-- TODO: Replace with your poster PDF -->
      <iframe  src="static/pdfs/paper.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@misc{zhang2025pneugelsightsoftroboticvisionbased,
      title={PneuGelSight: Soft Robotic Vision-Based Proprioception and Tactile Sensing}, 
      author={Ruohan Zhang and Uksang Yoo and Yichen Li and Arpit Argawal and Wenzhen Yuan},
      year={2025},
      eprint={2508.18443},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2508.18443}, 
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
